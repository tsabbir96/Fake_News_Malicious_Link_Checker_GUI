{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "E2bsoxg60ABm",
    "outputId": "fc41b456-a347-4eea-d93b-98503d72a234"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBfFRL0s6G_C"
   },
   "outputs": [],
   "source": [
    "\n",
    "def getTokens(input):\n",
    "\ttokensBySlash = str(input.encode('utf-8')).split('/')\t#get tokens after splitting by slash\n",
    "\tallTokens = []\n",
    "\tfor i in tokensBySlash:\n",
    "\t\ttokens = str(i).split('-')\t#get tokens after splitting by dash\n",
    "\t\ttokensByDot = []\n",
    "\t\tfor j in range(0,len(tokens)):\n",
    "\t\t\ttempTokens = str(tokens[j]).split('.')\t#get tokens after splitting by dot\n",
    "\t\t\ttokensByDot = tokensByDot + tempTokens\n",
    "\t\tallTokens = allTokens + tokens + tokensByDot\n",
    "\tallTokens = list(set(allTokens))\t#remove redundant tokens\n",
    "\tif 'com' in allTokens:\n",
    "\t\tallTokens.remove('com')\t#removing .com since it occurs a lot of times and it should not be included in our features\n",
    "\treturn allTokens\n",
    "\n",
    "\n",
    "    \n",
    "#this function scraps the article fromt the given link\n",
    "def scrapper(url):\n",
    "    #send request to the url\n",
    "    r1 = requests.get(url)\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    #Get all content that has \"<p>\" html tags\n",
    "    article = soup1.find_all('p')\n",
    "    content = []\n",
    "    list_paragraphs = []\n",
    "    #append all the paragraphs into one single paragraph\n",
    "    for p in np.arange(0, len(article)):\n",
    "        paragraph = article[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "    content.append(final_article)\n",
    "    \n",
    "    #now time to scrap the title\n",
    "    #Get all content that has \"<h1>\" or \"\"<h2>\" html tags\n",
    "    titles = soup1.find_all(re.compile(r'(h1|h2)'))  \n",
    "    \n",
    "    #Now a believe a title less than 30 characters is not a title.. it might be a outlier\n",
    "    #so i need to pick one that hase more than 30 characters\n",
    "    for t in titles:\n",
    "        length = len(str(t.get_text()))\n",
    "        if length >= 30:\n",
    "            title = t.get_text()\n",
    "            break\n",
    "\n",
    "    # Now once I have the title, url, article.. I create a dataframe with all these\n",
    "    df = pd.DataFrame(\n",
    "        {'url': url,\n",
    "         'headline': title,\n",
    "         'body': content})\n",
    "    return (df)\n",
    "\n",
    "\n",
    "#This function predicts the article real or fake\n",
    "def news_classifier(data):\n",
    "    #transform all the characters to lower form. Because python thinks \"HAS\" and \"has\" is not same\n",
    "    data['body'] = [entry.lower() for entry in data['body']]\n",
    "    #Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    data['body']= [word_tokenize(entry) for entry in data['body']]\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    i=0\n",
    "    for index,entry in enumerate(data['body']):\n",
    "\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        data.loc[index,'text_final'] = str(Final_words)\n",
    "    \n",
    "    #These will now contain for each row a list of unique integer number \n",
    "    #and its associated importance as calculated by TF-IDF\n",
    "    X = tokenizer_news.transform(data['text_final'])\n",
    "    predictions = nb.predict(X)\n",
    "    \n",
    "    if predictions == 0:\n",
    "        return(\"FAKE\")\n",
    "    \n",
    "    if predictions == 1:\n",
    "        return(\"REAL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZdQwWix1PwL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.2.post1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.22.2.post1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Unpickle files\n",
    "\n",
    "#get pickled file for url classifier\n",
    "lr = open(\"./LogReg_url.sav\",\"rb\")\n",
    "lr = pickle.load(lr)\n",
    "\n",
    "#get pickled file for url tokenizer\n",
    "Tfidf_vect = open(\"tokenizer.sav\",\"rb\")\n",
    "Tfidf_vect = pickle.load(Tfidf_vect)\n",
    "\n",
    "#get pickled file for news classifier\n",
    "nb = open(\"./naive_news.sav\",\"rb\")\n",
    "nb = pickle.load(nb)\n",
    "\n",
    "#get pickled file for news tokenizer\n",
    "tokenizer_news = open(\"./tokenizer_news.sav\",\"rb\")\n",
    "tokenizer_news = pickle.load(tokenizer_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0faI9D3_BMz"
   },
   "source": [
    "# GUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "cdAY4ZRT_I3A",
    "outputId": "7f5334e8-ea31-4e95-8f5b-b708036df54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: \n",
      "\n",
      " https://www.bbc.com/news/health-53665008\n",
      "\n",
      "\n",
      "Title: \n",
      "\n",
      " Coronavirus: Asymptomatic cases 'carry same amount of virus'\n",
      "\n",
      "\n",
      "Body: \n",
      "\n",
      "  Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp LinkedIn Copy this link These are external links and will open in a new window People with symptomless Covid-19 can carry as much of the virus as those with symptoms, a South Korean study has suggested. South Korea was able to identify and isolate asymptomatic cases through mass testing as early as the start of March.  There is mounting evidence these cases represent a considerable proportion of coronavirus infections. But the researchers weren't able to say how much these people actually passed the virus on. People with a positive coronavirus test were monitored in a community treatment centre, allowing scientists to look at how much of the virus was detectable in their nose and throat swabs. They were given regular tests, and only released once they were negative.  Results of 1,886 tests suggest people with no symptoms at the time of the test, including those who never go on to develop symptoms, have the same amount of viral material in their nose and throat as people with symptoms. The study also showed the virus could be detected in asymptomatic people for significant periods of time - although they appeared to clear it from their systems slightly faster than people with symptoms.  The median time (the number where half of cases were higher and half were lower) from being diagnosed to receiving a negative test was 17 days in asymptomatic patients and 19.5 days in symptomatic patients.  Because of the nature of the isolation centre, the study didn't include people with severe cases of the disease. They were also younger and healthier than average.  Most coronavirus testing, including in the UK, focuses on people with symptoms, so there is little data on asymptomatic cases. This study gives us some more information about what they look like in the body. The researchers acknowledge their study could not \"determine the role\" that the presence of the virus in asymptomatic patients played in transmission, however.  In theory, having the same amount of virus in your nose and throat means you have just as much to pass on. But people without symptoms are less likely to have a hacking cough that will send infected droplets further into the air.  There is \"as much virus in their respiratory mucus as someone who has the disease\", says Dr Simon Clarke, a cellular microbiologist at the University of Reading.  But, he added, \"that doesn't mean they're spraying as much into the environment\". While there was still a risk from people without symptoms, someone with symptoms who was \"coughing and spraying out the virus\" was likely to be a higher risk, he said. The risks of catching coronavirus from anyone depended on a number of factors, said infection biologist Dr Andrew Preston from the University of Bath. That included how deeply and quickly the infected person was breathing, how close you were to them for how long, and whether or not you were in a closed environment, he added. Rescue and clean up operations are under way, as residents blame government negligence for the blast. Have you been getting these songs wrong? What happens to your body in extreme heat?\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "#create window for url checker\n",
    "root= tk.Tk()\n",
    "root.title(\"URL Checker\")\n",
    "\n",
    "#create canvas\n",
    "canvas1 = tk.Canvas(root, width = 400, height = 300)\n",
    "canvas1.pack()\n",
    "\n",
    "#create label\n",
    "label3 = tk.Label(root, text= \"Enter URL:\")\n",
    "canvas1.create_window(200, 110, window=label3)\n",
    "\n",
    "#create entry\n",
    "entry1 = tk.Entry (root) \n",
    "canvas1.create_window(200, 140, window=entry1)\n",
    "\n",
    "#create function for url checker button\n",
    "def urlchecker():  \n",
    "    #gets the text from user input\n",
    "    url = entry1.get()\n",
    "    #converts it to lower form\n",
    "    val= url.lower()\n",
    "    val=[val]\n",
    "    \n",
    "    #tokenize the input\n",
    "    X = Tfidf_vect.transform(val)\n",
    "    predictions_LR = lr.predict(X)\n",
    "    \n",
    "    if predictions_LR == 0:\n",
    "        predictions_LR = \"Not malicious\"\n",
    "        label1 = tk.Label(root, bg=\"#90ee90\", fg=\"green\",text= predictions_LR)\n",
    "        canvas1.create_window(200, 230, window=label1)\n",
    "        \n",
    "        #scrap the data\n",
    "        data = scrapper(url)\n",
    "        \n",
    "        #convert pandas dataframe to numpy array\n",
    "        arr = data.to_numpy()\n",
    "        arr = arr.flatten()\n",
    "        \n",
    "        #get url\n",
    "        u = arr[0]\n",
    "        #get title\n",
    "        t = arr[1]\n",
    "        #get body\n",
    "        b = arr[2]\n",
    "        \n",
    "        print(\"URL: \\n\\n\",u)\n",
    "        print(\"\\n\\nTitle: \\n\\n\",t)\n",
    "        print(\"\\n\\nBody: \\n\\n\",b)\n",
    "        \n",
    "        #create window for url checker\n",
    "        root2= tk.Tk()\n",
    "        root2.title(\"News Checker\")\n",
    "        \n",
    "        #create canvas\n",
    "        canvas2 = tk.Canvas(root2, width = 400, height = 300)\n",
    "        canvas2.pack()\n",
    "        \n",
    "        #create label\n",
    "        label4 = tk.Label(root2, text= \"Enter Article Name:\")\n",
    "        canvas2.create_window(200, 110, window=label4)\n",
    "        \n",
    "        #create entry\n",
    "        entry2 = tk.Entry (root2) \n",
    "        canvas2.create_window(200, 140, window=entry2)\n",
    "        \n",
    "        #create function for news_checker button\n",
    "        def newschecker(t,data):\n",
    "            \n",
    "            #gets the text from user input\n",
    "            title = entry2.get()\n",
    "            #converts to lower form\n",
    "            title = title.lower()\n",
    "            \n",
    "            #check the title user inputed is present in the scrapped database\n",
    "            if title == t.lower():\n",
    "                #classify the article\n",
    "                news_prediction = news_classifier(data)\n",
    "                if news_prediction == \"REAL\":\n",
    "                    label2 = tk.Label(root2,bg=\"#90ee90\", fg=\"green\", text= \"The news is \" + str(news_prediction) + \"!!!!\")\n",
    "                    canvas2.create_window(200, 230, window=label2)\n",
    "                if news_prediction == \"FAKE\":\n",
    "                    label2 = tk.Label(root2,bg=\"#ffcccb\", fg=\"red\", text= \"The news is \" + str(news_prediction) + \"!!!!\")\n",
    "                    canvas2.create_window(200, 230, window=label2)\n",
    "            else:\n",
    "                label2 = tk.Label(root2, text= \"No article found of this name\")\n",
    "                canvas2.create_window(200, 230, window=label2)\n",
    "        \n",
    "        #button for checking news article\n",
    "        button2 = tk.Button(master=root2,text='Check article', command=lambda: newschecker(t,data))\n",
    "        canvas2.create_window(200, 180, window=button2)\n",
    "\n",
    "        root2.mainloop()\n",
    "                     \n",
    "    \n",
    "    if predictions_LR == 1:\n",
    "        predictions_LR = \"The URL is malicious. Please do not proceed\"\n",
    "        label1 = tk.Label(root, bg=\"#ffcccb\", fg=\"red\", text= predictions_LR)\n",
    "        canvas1.create_window(200, 230, window=label1)\n",
    "    \n",
    "#button for checking url   \n",
    "button1 = tk.Button(text='Check URL', command=urlchecker)\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Url checker_Gui.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
